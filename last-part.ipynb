{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelBinarizer loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "mlb = joblib.load(\"mlb.pkl\")\n",
    "print(\"MultiLabelBinarizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded!\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the Whisper model\n",
    "\n",
    "# # in gpu\n",
    "# whisper_model = whisper.load_model(\"small\").to(\"cuda\")\n",
    "# # Check if the model is on GPU\n",
    "# print(next(whisper_model.parameters()).device)  # Should print \"cuda:0\"\n",
    "\n",
    "whisper_model = whisper.load_model(\"small\").to(\"cpu\")\n",
    "\n",
    "print(\"Whisper model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Roaming\\Python\\Python312\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  अद्रस्पोर्ट् कोन, हैश भेशर्मी और हैश भेईमानी की इंतिहांतो देकिए ज़रा, ये आद्मेश जो दोहाँसाद के जुदुसो में इस तरे नाच्ता ता, आज खुद को चत्रपती शिवाजी महाराच का विरासब्ट का चेकेडार बताता है.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transcribe Hindi audio\n",
    "#31\n",
    "#33\n",
    "audio_path = \"C:/Users/deepa/OneDrive/Desktop/Spechaudio/vishnu_31.mp3\"\n",
    "result = whisper_model.transcribe(audio_path, language=\"hi\")\n",
    "\n",
    "# Extract transcribed text\n",
    "transcribed_text = result[\"text\"]\n",
    "print(\"Transcribed Text:\", transcribed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: अदरसपरट कन हश भशरम और हश भईमन क इतहत दकए जर य आदमश ज दहसद क जदस म इस तर नचत त आज खद क चतरपत शवज महरच क वरसबट क चकडर बतत ह\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs and punctuation, then extra spaces\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "cleaned_text = clean_text(transcribed_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer for IndicBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Convert cleaned text into token IDs\n",
    "tokens = tokenizer(cleaned_text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")['input_ids']\n",
    "tokens = tokens.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentence Type: [('fake', 'hate')]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define Hybrid Model class\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.conv1 = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(input_ids).last_hidden_state\n",
    "        conv1_out = self.relu(self.conv1(bert_output.permute(0, 2, 1)))\n",
    "        conv2_out = self.relu(self.conv2(conv1_out))\n",
    "        pooled = torch.mean(conv2_out, dim=2)\n",
    "        output = self.sigmoid(self.fc(pooled))\n",
    "        return output\n",
    "\n",
    "# Load BERT model\n",
    "from transformers import AutoModel\n",
    "\n",
    "indic_bert = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Define and load the trained model\n",
    "num_classes = 5  # Change this based on your number of sentence types\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = HybridModel(indic_bert, num_classes)\n",
    "model.load_state_dict(torch.load(r\"./nlp_model/best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Predict the sentence type\n",
    "with torch.no_grad():\n",
    "    output = model(tokens)\n",
    "\n",
    "# Convert prediction to labels\n",
    "predicted_labels = (output.cpu().numpy() > 0.5).astype(int)  # Binary thresholding\n",
    "print(\"Predicted Sentence Type:\", mlb.inverse_transform(predicted_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
